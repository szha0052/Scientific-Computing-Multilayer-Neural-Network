{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b954ab5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from MLP.model import MLP\n",
    "import pandas as pd\n",
    "from MLP.layers import SoftmaxCrossEntropy\n",
    "from MLP.layers import Adam\n",
    "from MLP.layers import MiniBatchFit\n",
    "from MLP.layers import SGDMomentum\n",
    "\n",
    "# 导入npy数据集\n",
    "X_train = np.load('Assignment1-Dataset/train_data.npy')\n",
    "X_test = np.load('Assignment1-Dataset/test_data.npy')\n",
    "Y_train = np.load('Assignment1-Dataset/train_label.npy').flatten()\n",
    "Y_test = np.load('Assignment1-Dataset/test_label.npy').flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00ce8560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 128), (50000,), (10000, 128), (10000,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fe16aa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=uint8)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计数据集的类别\n",
    "classes = np.unique(Y_train)\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e162212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(X, y, test_size=0.2, random_seed=None):\n",
    "    \"\"\"\n",
    "    使用 NumPy 实现的分层抽样训练-测试数据划分。\n",
    "    \n",
    "    :param X: 特征数组，形状 (N, d)，N为样本数，d为特征维度\n",
    "    :param y: 标签数组，形状 (N, )\n",
    "    :param test_size: 测试集比例（0.0 ~ 1.0）\n",
    "    :param random_seed: 随机数种子，用于可复现的结果\n",
    "    :return: (X_train, X_test, y_train, y_test)\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    # 找到标签中所有不同类别，以及每个类别出现的索引\n",
    "    unique_labels = np.unique(y)\n",
    "\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # 找到该类别所有样本的索引\n",
    "        label_indices = np.where(y == label)[0]\n",
    "        \n",
    "        # 打乱该类别索引，防止顺序影响\n",
    "        np.random.shuffle(label_indices)\n",
    "\n",
    "        # 计算应该分配到测试集的数量\n",
    "        test_count = int(len(label_indices) * test_size)\n",
    "        \n",
    "        # 前 test_count 个样本分到测试集，其余分到训练集\n",
    "        test_indices.append(label_indices[:test_count])\n",
    "        train_indices.append(label_indices[test_count:])\n",
    "\n",
    "    # 拼接所有类别的索引\n",
    "    train_indices = np.concatenate(train_indices)\n",
    "    test_indices = np.concatenate(test_indices)\n",
    "\n",
    "    # 如果需要进一步打乱train和test索引，可以再做一次shuffle\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "\n",
    "    # 根据索引切分数据\n",
    "    X_train, X_test = X[train_indices], X[test_indices]\n",
    "    y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ad832b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_numpy(y_true, y_pred, average='macro'):\n",
    "    \"\"\"\n",
    "    计算多分类 Precision、Recall 和 F1，仅使用 NumPy。\n",
    "    \n",
    "    :param y_true: 真实标签，形状 (N,)\n",
    "    :param y_pred: 预测标签，形状 (N,)\n",
    "    :param average: 指定平均方式，可选 'macro' 或 'micro'\n",
    "    :return: (precision, recall, f1)\n",
    "             - 如果 average='macro'，返回宏平均的 (P, R, F1)\n",
    "             - 如果 average='micro'，返回微平均的 (P, R, F1)\n",
    "    \"\"\"\n",
    "    # 收集所有类别，并建立索引映射（如果需要）\n",
    "    classes = np.unique(np.concatenate((y_true, y_pred)))\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    # 构建混淆矩阵 confusion matrix\n",
    "    #  行：真实标签\n",
    "    #  列：预测标签\n",
    "    conf_mat = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "\n",
    "    # 如果你的标签不是从0开始的连续整数，需要建立映射\n",
    "    # 这里假设 classes 已经是从小到大的唯一整数标签\n",
    "    # 构建 label -> index 的映射字典\n",
    "    label_to_index = {label: idx for idx, label in enumerate(classes)}\n",
    "    \n",
    "    for t, p in zip(y_true, y_pred):\n",
    "        i = label_to_index[t]\n",
    "        j = label_to_index[p]\n",
    "        conf_mat[i, j] += 1\n",
    "\n",
    "    if average == 'macro':\n",
    "        # 宏平均：对每个类别计算后再平均\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        f1s = []\n",
    "        for i in range(num_classes):\n",
    "            TP = conf_mat[i, i]\n",
    "            FP = conf_mat[:, i].sum() - TP  # 该列除对角线外的元素之和\n",
    "            FN = conf_mat[i, :].sum() - TP  # 该行除对角线外的元素之和\n",
    "\n",
    "            # 注意分母可能为 0，要做检查\n",
    "            prec = TP / (TP + FP) if (TP + FP) != 0 else 0.0\n",
    "            rec = TP / (TP + FN) if (TP + FN) != 0 else 0.0\n",
    "            f1 = 2 * prec * rec / (prec + rec) if (prec + rec) != 0 else 0.0\n",
    "\n",
    "            precisions.append(prec)\n",
    "            recalls.append(rec)\n",
    "            f1s.append(f1)\n",
    "        \n",
    "        precision_macro = np.mean(precisions)\n",
    "        recall_macro = np.mean(recalls)\n",
    "        f1_macro = np.mean(f1s)\n",
    "        return precision_macro, recall_macro, f1_macro\n",
    "\n",
    "    elif average == 'micro':\n",
    "        # 微平均：基于全局的 TP, FP, FN 来计算\n",
    "        TP_total = np.diag(conf_mat).sum()\n",
    "        # 对每列：列和 - 对角线元素，即所有列上的假正例\n",
    "        FP_total = conf_mat.sum(axis=0) - np.diag(conf_mat)\n",
    "        # 对每行：行和 - 对角线元素，即所有行上的假负例\n",
    "        FN_total = conf_mat.sum(axis=1) - np.diag(conf_mat)\n",
    "        \n",
    "        FP_total = FP_total.sum()\n",
    "        FN_total = FN_total.sum()\n",
    "\n",
    "        precision_micro = TP_total / (TP_total + FP_total) if (TP_total + FP_total) != 0 else 0.0\n",
    "        recall_micro = TP_total / (TP_total + FN_total) if (TP_total + FN_total) != 0 else 0.0\n",
    "        f1_micro = 2 * precision_micro * recall_micro / (precision_micro + recall_micro) \\\n",
    "            if (precision_micro + recall_micro) != 0 else 0.0\n",
    "        \n",
    "        return precision_micro, recall_micro, f1_micro\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported average type. Use 'macro' or 'micro'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "054dcf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000, 128), (40000,), (10000, 128), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, Y_train, Y_val = stratified_train_test_split(X_train, Y_train, test_size=0.2, random_seed=42)\n",
    "X_train.shape, Y_train.shape, X_test.shape, Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd0a02a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6    4000\n",
       " 7    4000\n",
       " 8    4000\n",
       " 3    4000\n",
       " 2    4000\n",
       " 1    4000\n",
       " 4    4000\n",
       " 9    4000\n",
       " 0    4000\n",
       " 5    4000\n",
       " Name: count, dtype: int64,\n",
       " 3    1000\n",
       " 8    1000\n",
       " 0    1000\n",
       " 6    1000\n",
       " 1    1000\n",
       " 9    1000\n",
       " 5    1000\n",
       " 7    1000\n",
       " 4    1000\n",
       " 2    1000\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 每个数据集的种类样本数\n",
    "pd.Series(Y_train).value_counts(), pd.Series(Y_test).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa98f263",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 超参数\n",
    "input_dim = 128  # 输入数据维度\n",
    "hidden_dims = [64, 32]  # 隐藏层维度\n",
    "output_dim = 10  # 输出类别数（例如10分类）\n",
    "learning_rate = 0.001\n",
    "momentum = 0.9\n",
    "num_epochs = 200\n",
    "batch_size = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afe43ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 2.0218\n",
      "Epoch 2/200, Loss: 1.8389\n",
      "Epoch 3/200, Loss: 1.7927\n",
      "Epoch 4/200, Loss: 1.7667\n",
      "Epoch 5/200, Loss: 1.7467\n",
      "Epoch 6/200, Loss: 1.7297\n",
      "Epoch 7/200, Loss: 1.7168\n",
      "Epoch 8/200, Loss: 1.7045\n",
      "Epoch 9/200, Loss: 1.6943\n",
      "Epoch 10/200, Loss: 1.6850\n",
      "Epoch 11/200, Loss: 1.6719\n",
      "Epoch 12/200, Loss: 1.6684\n",
      "Epoch 13/200, Loss: 1.6582\n",
      "Epoch 14/200, Loss: 1.6497\n",
      "Epoch 15/200, Loss: 1.6416\n",
      "Epoch 16/200, Loss: 1.6337\n",
      "Epoch 17/200, Loss: 1.6320\n",
      "Epoch 18/200, Loss: 1.6239\n",
      "Epoch 19/200, Loss: 1.6219\n",
      "Epoch 20/200, Loss: 1.6188\n",
      "Epoch 21/200, Loss: 1.6131\n",
      "Epoch 22/200, Loss: 1.6102\n",
      "Epoch 23/200, Loss: 1.6014\n",
      "Epoch 24/200, Loss: 1.6001\n",
      "Epoch 25/200, Loss: 1.5924\n",
      "Epoch 26/200, Loss: 1.5881\n",
      "Epoch 27/200, Loss: 1.5825\n",
      "Epoch 28/200, Loss: 1.5771\n",
      "Epoch 29/200, Loss: 1.5776\n",
      "Epoch 30/200, Loss: 1.5744\n",
      "Epoch 31/200, Loss: 1.5718\n",
      "Epoch 32/200, Loss: 1.5614\n",
      "Epoch 33/200, Loss: 1.5630\n",
      "Epoch 34/200, Loss: 1.5552\n",
      "Epoch 35/200, Loss: 1.5557\n",
      "Epoch 36/200, Loss: 1.5503\n",
      "Epoch 37/200, Loss: 1.5491\n",
      "Epoch 38/200, Loss: 1.5453\n",
      "Epoch 39/200, Loss: 1.5369\n",
      "Epoch 40/200, Loss: 1.5375\n",
      "Epoch 41/200, Loss: 1.5334\n",
      "Epoch 42/200, Loss: 1.5288\n",
      "Epoch 43/200, Loss: 1.5249\n",
      "Epoch 44/200, Loss: 1.5251\n",
      "Epoch 45/200, Loss: 1.5177\n",
      "Epoch 46/200, Loss: 1.5183\n",
      "Epoch 47/200, Loss: 1.5147\n",
      "Epoch 48/200, Loss: 1.5153\n",
      "Epoch 49/200, Loss: 1.5115\n",
      "Epoch 50/200, Loss: 1.5119\n",
      "Epoch 51/200, Loss: 1.5070\n",
      "Epoch 52/200, Loss: 1.5056\n",
      "Epoch 53/200, Loss: 1.5075\n",
      "Epoch 54/200, Loss: 1.5043\n",
      "Epoch 55/200, Loss: 1.5007\n",
      "Epoch 56/200, Loss: 1.4951\n",
      "Epoch 57/200, Loss: 1.4950\n",
      "Epoch 58/200, Loss: 1.4959\n",
      "Epoch 59/200, Loss: 1.4917\n",
      "Epoch 60/200, Loss: 1.4891\n",
      "Epoch 61/200, Loss: 1.4838\n",
      "Epoch 62/200, Loss: 1.4886\n",
      "Epoch 63/200, Loss: 1.4835\n",
      "Epoch 64/200, Loss: 1.4807\n",
      "Epoch 65/200, Loss: 1.4797\n",
      "Epoch 66/200, Loss: 1.4791\n",
      "Epoch 67/200, Loss: 1.4762\n",
      "Epoch 68/200, Loss: 1.4756\n",
      "Epoch 69/200, Loss: 1.4707\n",
      "Epoch 70/200, Loss: 1.4716\n",
      "Epoch 71/200, Loss: 1.4674\n",
      "Epoch 72/200, Loss: 1.4666\n",
      "Epoch 73/200, Loss: 1.4653\n",
      "Epoch 74/200, Loss: 1.4662\n",
      "Epoch 75/200, Loss: 1.4678\n",
      "Epoch 76/200, Loss: 1.4611\n",
      "Epoch 77/200, Loss: 1.4616\n",
      "Epoch 78/200, Loss: 1.4649\n",
      "Epoch 79/200, Loss: 1.4592\n",
      "Epoch 80/200, Loss: 1.4602\n",
      "Epoch 81/200, Loss: 1.4561\n",
      "Epoch 82/200, Loss: 1.4567\n",
      "Epoch 83/200, Loss: 1.4567\n",
      "Epoch 84/200, Loss: 1.4576\n",
      "Epoch 85/200, Loss: 1.4496\n",
      "Epoch 86/200, Loss: 1.4543\n",
      "Epoch 87/200, Loss: 1.4527\n",
      "Epoch 88/200, Loss: 1.4483\n",
      "Epoch 89/200, Loss: 1.4480\n",
      "Epoch 90/200, Loss: 1.4465\n",
      "Epoch 91/200, Loss: 1.4466\n",
      "Epoch 92/200, Loss: 1.4479\n",
      "Epoch 93/200, Loss: 1.4456\n",
      "Epoch 94/200, Loss: 1.4419\n",
      "Epoch 95/200, Loss: 1.4465\n",
      "Epoch 96/200, Loss: 1.4448\n",
      "Epoch 97/200, Loss: 1.4458\n",
      "Epoch 98/200, Loss: 1.4403\n",
      "Epoch 99/200, Loss: 1.4380\n",
      "Epoch 100/200, Loss: 1.4420\n",
      "Epoch 101/200, Loss: 1.4412\n",
      "Epoch 102/200, Loss: 1.4364\n",
      "Epoch 103/200, Loss: 1.4321\n",
      "Epoch 104/200, Loss: 1.4379\n",
      "Epoch 105/200, Loss: 1.4409\n",
      "Epoch 106/200, Loss: 1.4324\n",
      "Epoch 107/200, Loss: 1.4356\n",
      "Epoch 108/200, Loss: 1.4289\n",
      "Epoch 109/200, Loss: 1.4330\n",
      "Epoch 110/200, Loss: 1.4376\n",
      "Epoch 111/200, Loss: 1.4302\n",
      "Epoch 112/200, Loss: 1.4347\n",
      "Epoch 113/200, Loss: 1.4291\n",
      "Epoch 114/200, Loss: 1.4323\n",
      "Epoch 115/200, Loss: 1.4309\n",
      "Epoch 116/200, Loss: 1.4287\n",
      "Epoch 117/200, Loss: 1.4246\n",
      "Epoch 118/200, Loss: 1.4317\n",
      "Epoch 119/200, Loss: 1.4283\n",
      "Epoch 120/200, Loss: 1.4262\n",
      "Epoch 121/200, Loss: 1.4253\n",
      "Epoch 122/200, Loss: 1.4263\n",
      "Epoch 123/200, Loss: 1.4259\n",
      "Epoch 124/200, Loss: 1.4229\n",
      "Epoch 125/200, Loss: 1.4244\n",
      "Epoch 126/200, Loss: 1.4236\n",
      "Epoch 127/200, Loss: 1.4221\n",
      "Epoch 128/200, Loss: 1.4232\n",
      "Epoch 129/200, Loss: 1.4166\n",
      "Epoch 130/200, Loss: 1.4201\n",
      "Epoch 131/200, Loss: 1.4204\n",
      "Epoch 132/200, Loss: 1.4231\n",
      "Epoch 133/200, Loss: 1.4270\n",
      "Epoch 134/200, Loss: 1.4228\n",
      "Epoch 135/200, Loss: 1.4177\n",
      "Epoch 136/200, Loss: 1.4182\n",
      "Epoch 137/200, Loss: 1.4181\n",
      "Epoch 138/200, Loss: 1.4184\n",
      "Epoch 139/200, Loss: 1.4169\n",
      "Epoch 140/200, Loss: 1.4147\n",
      "Epoch 141/200, Loss: 1.4158\n",
      "Epoch 142/200, Loss: 1.4187\n",
      "Epoch 143/200, Loss: 1.4156\n",
      "Epoch 144/200, Loss: 1.4182\n",
      "Epoch 145/200, Loss: 1.4161\n",
      "Epoch 146/200, Loss: 1.4116\n",
      "Epoch 147/200, Loss: 1.4168\n",
      "Epoch 148/200, Loss: 1.4131\n",
      "Epoch 149/200, Loss: 1.4167\n",
      "Epoch 150/200, Loss: 1.4128\n",
      "Epoch 151/200, Loss: 1.4114\n",
      "Epoch 152/200, Loss: 1.4091\n",
      "Epoch 153/200, Loss: 1.4051\n",
      "Epoch 154/200, Loss: 1.4137\n",
      "Epoch 155/200, Loss: 1.4145\n",
      "Epoch 156/200, Loss: 1.4109\n",
      "Epoch 157/200, Loss: 1.4094\n",
      "Epoch 158/200, Loss: 1.4084\n",
      "Epoch 159/200, Loss: 1.4078\n",
      "Epoch 160/200, Loss: 1.4066\n",
      "Epoch 161/200, Loss: 1.4044\n",
      "Epoch 162/200, Loss: 1.4043\n",
      "Epoch 163/200, Loss: 1.4064\n",
      "Epoch 164/200, Loss: 1.4070\n",
      "Epoch 165/200, Loss: 1.3980\n",
      "Epoch 166/200, Loss: 1.4081\n",
      "Epoch 167/200, Loss: 1.4097\n",
      "Epoch 168/200, Loss: 1.4070\n",
      "Epoch 169/200, Loss: 1.4020\n",
      "Epoch 170/200, Loss: 1.4004\n",
      "Epoch 171/200, Loss: 1.4037\n",
      "Epoch 172/200, Loss: 1.4050\n",
      "Epoch 173/200, Loss: 1.4082\n",
      "Epoch 174/200, Loss: 1.4052\n",
      "Epoch 175/200, Loss: 1.4030\n",
      "Epoch 176/200, Loss: 1.4023\n",
      "Epoch 177/200, Loss: 1.4024\n",
      "Epoch 178/200, Loss: 1.4070\n",
      "Epoch 179/200, Loss: 1.4018\n",
      "Epoch 180/200, Loss: 1.3947\n",
      "Epoch 181/200, Loss: 1.3972\n",
      "Epoch 182/200, Loss: 1.3984\n",
      "Epoch 183/200, Loss: 1.4021\n",
      "Epoch 184/200, Loss: 1.4046\n",
      "Epoch 185/200, Loss: 1.3945\n",
      "Epoch 186/200, Loss: 1.4028\n",
      "Epoch 187/200, Loss: 1.4005\n",
      "Epoch 188/200, Loss: 1.3949\n",
      "Epoch 189/200, Loss: 1.4021\n",
      "Epoch 190/200, Loss: 1.4020\n",
      "Epoch 191/200, Loss: 1.4017\n",
      "Epoch 192/200, Loss: 1.3951\n",
      "Epoch 193/200, Loss: 1.3900\n",
      "Epoch 194/200, Loss: 1.4007\n",
      "Epoch 195/200, Loss: 1.3926\n",
      "Epoch 196/200, Loss: 1.3928\n",
      "Epoch 197/200, Loss: 1.3918\n",
      "Epoch 198/200, Loss: 1.4012\n",
      "Epoch 199/200, Loss: 1.3984\n",
      "Epoch 200/200, Loss: 1.3966\n"
     ]
    }
   ],
   "source": [
    "optimizer1=Adam()\n",
    "# 创建MLP模型\n",
    "model_Adam = MLP(input_dim, hidden_dims, output_dim, dropout_rate=0.2, weight_decay=1e-4, optimizer=optimizer1)\n",
    "# 定义损失函数\n",
    "\n",
    "\n",
    "Adam_Fit = MiniBatchFit(model_Adam, optimizer1,\n",
    "             X_train, Y_train, \n",
    "             output_dim, \n",
    "             num_epochs=num_epochs, \n",
    "             batch_size=batch_size, \n",
    "             learning_rate=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "Adam_Fit.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0aeccea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 50.02%\n",
      "Validation F1 Score: 0.4867\n",
      "Validation Recall: 0.5002\n",
      "Validation Precision: 0.5061\n"
     ]
    }
   ],
   "source": [
    "predictions_train_Adam = model_Adam.predict_and_evaluate(X_val, Y_val)\n",
    "precision_val,recall_val,f1_val = classification_report_numpy(Y_val, predictions_train_Adam, average='macro')\n",
    "print(f\"Validation F1 Score: {f1_val:.4f}\")\n",
    "print(f\"Validation Recall: {recall_val:.4f}\")\n",
    "print(f\"Validation Precision: {precision_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "516ebbc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 50.79%\n",
      "Test F1 Score: 0.4929\n",
      "Test Recall: 0.5079\n",
      "Test Precision: 0.5137\n"
     ]
    }
   ],
   "source": [
    "predictions_test_Adam = model_Adam.predict_and_evaluate(X_test, Y_test)\n",
    "precision_test,recall_test,f1_test = classification_report_numpy(Y_test, predictions_test_Adam, average='macro')\n",
    "print(f\"Test F1 Score: {f1_test:.4f}\")\n",
    "print(f\"Test Recall: {recall_test:.4f}\")\n",
    "print(f\"Test Precision: {precision_test:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d541ae2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: 2.3012\n",
      "Epoch 2/200, Loss: 2.1222\n",
      "Epoch 3/200, Loss: 2.0414\n",
      "Epoch 4/200, Loss: 1.9899\n",
      "Epoch 5/200, Loss: 1.9534\n",
      "Epoch 6/200, Loss: 1.9255\n",
      "Epoch 7/200, Loss: 1.9004\n",
      "Epoch 8/200, Loss: 1.8772\n",
      "Epoch 9/200, Loss: 1.8576\n",
      "Epoch 10/200, Loss: 1.8422\n",
      "Epoch 11/200, Loss: 1.8291\n",
      "Epoch 12/200, Loss: 1.8174\n",
      "Epoch 13/200, Loss: 1.8044\n",
      "Epoch 14/200, Loss: 1.7960\n",
      "Epoch 15/200, Loss: 1.7861\n",
      "Epoch 16/200, Loss: 1.7710\n",
      "Epoch 17/200, Loss: 1.7616\n",
      "Epoch 18/200, Loss: 1.7575\n",
      "Epoch 19/200, Loss: 1.7480\n",
      "Epoch 20/200, Loss: 1.7475\n",
      "Epoch 21/200, Loss: 1.7360\n",
      "Epoch 22/200, Loss: 1.7333\n",
      "Epoch 23/200, Loss: 1.7248\n",
      "Epoch 24/200, Loss: 1.7270\n",
      "Epoch 25/200, Loss: 1.7130\n",
      "Epoch 26/200, Loss: 1.7073\n",
      "Epoch 27/200, Loss: 1.7058\n",
      "Epoch 28/200, Loss: 1.6966\n",
      "Epoch 29/200, Loss: 1.6970\n",
      "Epoch 30/200, Loss: 1.6908\n",
      "Epoch 31/200, Loss: 1.6840\n",
      "Epoch 32/200, Loss: 1.6827\n",
      "Epoch 33/200, Loss: 1.6764\n",
      "Epoch 34/200, Loss: 1.6735\n",
      "Epoch 35/200, Loss: 1.6719\n",
      "Epoch 36/200, Loss: 1.6677\n",
      "Epoch 37/200, Loss: 1.6616\n",
      "Epoch 38/200, Loss: 1.6549\n",
      "Epoch 39/200, Loss: 1.6570\n",
      "Epoch 40/200, Loss: 1.6549\n",
      "Epoch 41/200, Loss: 1.6548\n",
      "Epoch 42/200, Loss: 1.6458\n",
      "Epoch 43/200, Loss: 1.6405\n",
      "Epoch 44/200, Loss: 1.6416\n",
      "Epoch 45/200, Loss: 1.6382\n",
      "Epoch 46/200, Loss: 1.6376\n",
      "Epoch 47/200, Loss: 1.6373\n",
      "Epoch 48/200, Loss: 1.6311\n",
      "Epoch 49/200, Loss: 1.6303\n",
      "Epoch 50/200, Loss: 1.6249\n",
      "Epoch 51/200, Loss: 1.6242\n",
      "Epoch 52/200, Loss: 1.6219\n",
      "Epoch 53/200, Loss: 1.6213\n",
      "Epoch 54/200, Loss: 1.6190\n",
      "Epoch 55/200, Loss: 1.6132\n",
      "Epoch 56/200, Loss: 1.6133\n",
      "Epoch 57/200, Loss: 1.6063\n",
      "Epoch 58/200, Loss: 1.6081\n",
      "Epoch 59/200, Loss: 1.6050\n",
      "Epoch 60/200, Loss: 1.6113\n",
      "Epoch 61/200, Loss: 1.6015\n",
      "Epoch 62/200, Loss: 1.6019\n",
      "Epoch 63/200, Loss: 1.5970\n",
      "Epoch 64/200, Loss: 1.5964\n",
      "Epoch 65/200, Loss: 1.5916\n",
      "Epoch 66/200, Loss: 1.5881\n",
      "Epoch 67/200, Loss: 1.5913\n",
      "Epoch 68/200, Loss: 1.5825\n",
      "Epoch 69/200, Loss: 1.5882\n",
      "Epoch 70/200, Loss: 1.5832\n",
      "Epoch 71/200, Loss: 1.5832\n",
      "Epoch 72/200, Loss: 1.5761\n",
      "Epoch 73/200, Loss: 1.5786\n",
      "Epoch 74/200, Loss: 1.5751\n",
      "Epoch 75/200, Loss: 1.5715\n",
      "Epoch 76/200, Loss: 1.5769\n",
      "Epoch 77/200, Loss: 1.5731\n",
      "Epoch 78/200, Loss: 1.5710\n",
      "Epoch 79/200, Loss: 1.5707\n",
      "Epoch 80/200, Loss: 1.5623\n",
      "Epoch 81/200, Loss: 1.5621\n",
      "Epoch 82/200, Loss: 1.5608\n",
      "Epoch 83/200, Loss: 1.5627\n",
      "Epoch 84/200, Loss: 1.5623\n",
      "Epoch 85/200, Loss: 1.5595\n",
      "Epoch 86/200, Loss: 1.5533\n",
      "Epoch 87/200, Loss: 1.5574\n",
      "Epoch 88/200, Loss: 1.5546\n",
      "Epoch 89/200, Loss: 1.5518\n",
      "Epoch 90/200, Loss: 1.5544\n",
      "Epoch 91/200, Loss: 1.5445\n",
      "Epoch 92/200, Loss: 1.5515\n",
      "Epoch 93/200, Loss: 1.5466\n",
      "Epoch 94/200, Loss: 1.5448\n",
      "Epoch 95/200, Loss: 1.5457\n",
      "Epoch 96/200, Loss: 1.5430\n",
      "Epoch 97/200, Loss: 1.5419\n",
      "Epoch 98/200, Loss: 1.5437\n",
      "Epoch 99/200, Loss: 1.5370\n",
      "Epoch 100/200, Loss: 1.5392\n",
      "Epoch 101/200, Loss: 1.5411\n",
      "Epoch 102/200, Loss: 1.5361\n",
      "Epoch 103/200, Loss: 1.5341\n",
      "Epoch 104/200, Loss: 1.5338\n",
      "Epoch 105/200, Loss: 1.5342\n",
      "Epoch 106/200, Loss: 1.5314\n",
      "Epoch 107/200, Loss: 1.5311\n",
      "Epoch 108/200, Loss: 1.5290\n",
      "Epoch 109/200, Loss: 1.5264\n",
      "Epoch 110/200, Loss: 1.5267\n",
      "Epoch 111/200, Loss: 1.5256\n",
      "Epoch 112/200, Loss: 1.5206\n",
      "Epoch 113/200, Loss: 1.5279\n",
      "Epoch 114/200, Loss: 1.5230\n",
      "Epoch 115/200, Loss: 1.5211\n",
      "Epoch 116/200, Loss: 1.5160\n",
      "Epoch 117/200, Loss: 1.5224\n",
      "Epoch 118/200, Loss: 1.5205\n",
      "Epoch 119/200, Loss: 1.5132\n",
      "Epoch 120/200, Loss: 1.5154\n",
      "Epoch 121/200, Loss: 1.5108\n",
      "Epoch 122/200, Loss: 1.5109\n",
      "Epoch 123/200, Loss: 1.5123\n",
      "Epoch 124/200, Loss: 1.5145\n",
      "Epoch 125/200, Loss: 1.5129\n",
      "Epoch 126/200, Loss: 1.5092\n",
      "Epoch 127/200, Loss: 1.5067\n",
      "Epoch 128/200, Loss: 1.5098\n",
      "Epoch 129/200, Loss: 1.5121\n",
      "Epoch 130/200, Loss: 1.5078\n",
      "Epoch 131/200, Loss: 1.5019\n",
      "Epoch 132/200, Loss: 1.5020\n",
      "Epoch 133/200, Loss: 1.5057\n",
      "Epoch 134/200, Loss: 1.5059\n",
      "Epoch 135/200, Loss: 1.5058\n",
      "Epoch 136/200, Loss: 1.5047\n",
      "Epoch 137/200, Loss: 1.5013\n",
      "Epoch 138/200, Loss: 1.5022\n",
      "Epoch 139/200, Loss: 1.4988\n",
      "Epoch 140/200, Loss: 1.4972\n",
      "Epoch 141/200, Loss: 1.4966\n",
      "Epoch 142/200, Loss: 1.5013\n",
      "Epoch 143/200, Loss: 1.4952\n",
      "Epoch 144/200, Loss: 1.4984\n",
      "Epoch 145/200, Loss: 1.4966\n",
      "Epoch 146/200, Loss: 1.4905\n",
      "Epoch 147/200, Loss: 1.4925\n",
      "Epoch 148/200, Loss: 1.4913\n",
      "Epoch 149/200, Loss: 1.4902\n",
      "Epoch 150/200, Loss: 1.4907\n",
      "Epoch 151/200, Loss: 1.4892\n",
      "Epoch 152/200, Loss: 1.4899\n",
      "Epoch 153/200, Loss: 1.4888\n",
      "Epoch 154/200, Loss: 1.4837\n",
      "Epoch 155/200, Loss: 1.4897\n",
      "Epoch 156/200, Loss: 1.4833\n",
      "Epoch 157/200, Loss: 1.4834\n",
      "Epoch 158/200, Loss: 1.4864\n",
      "Epoch 159/200, Loss: 1.4861\n",
      "Epoch 160/200, Loss: 1.4792\n",
      "Epoch 161/200, Loss: 1.4848\n",
      "Epoch 162/200, Loss: 1.4841\n",
      "Epoch 163/200, Loss: 1.4852\n",
      "Epoch 164/200, Loss: 1.4785\n",
      "Epoch 165/200, Loss: 1.4821\n",
      "Epoch 166/200, Loss: 1.4828\n",
      "Epoch 167/200, Loss: 1.4831\n",
      "Epoch 168/200, Loss: 1.4799\n",
      "Epoch 169/200, Loss: 1.4766\n",
      "Epoch 170/200, Loss: 1.4742\n",
      "Epoch 171/200, Loss: 1.4857\n",
      "Epoch 172/200, Loss: 1.4765\n",
      "Epoch 173/200, Loss: 1.4796\n",
      "Epoch 174/200, Loss: 1.4753\n",
      "Epoch 175/200, Loss: 1.4766\n",
      "Epoch 176/200, Loss: 1.4728\n",
      "Epoch 177/200, Loss: 1.4745\n",
      "Epoch 178/200, Loss: 1.4768\n",
      "Epoch 179/200, Loss: 1.4739\n",
      "Epoch 180/200, Loss: 1.4736\n",
      "Epoch 181/200, Loss: 1.4728\n",
      "Epoch 182/200, Loss: 1.4747\n",
      "Epoch 183/200, Loss: 1.4717\n",
      "Epoch 184/200, Loss: 1.4668\n",
      "Epoch 185/200, Loss: 1.4680\n",
      "Epoch 186/200, Loss: 1.4679\n",
      "Epoch 187/200, Loss: 1.4714\n",
      "Epoch 188/200, Loss: 1.4701\n",
      "Epoch 189/200, Loss: 1.4663\n",
      "Epoch 190/200, Loss: 1.4640\n",
      "Epoch 191/200, Loss: 1.4642\n",
      "Epoch 192/200, Loss: 1.4668\n",
      "Epoch 193/200, Loss: 1.4625\n",
      "Epoch 194/200, Loss: 1.4626\n",
      "Epoch 195/200, Loss: 1.4659\n",
      "Epoch 196/200, Loss: 1.4599\n",
      "Epoch 197/200, Loss: 1.4655\n",
      "Epoch 198/200, Loss: 1.4560\n",
      "Epoch 199/200, Loss: 1.4667\n",
      "Epoch 200/200, Loss: 1.4672\n"
     ]
    }
   ],
   "source": [
    "optimizer2 = SGDMomentum(momentum=0.9)\n",
    "\n",
    "# 创建MLP模型\n",
    "model_SGDM = MLP(input_dim, hidden_dims, output_dim, dropout_rate=0.2, weight_decay=1e-4, optimizer=optimizer2)\n",
    "\n",
    "SGDM_Fit = MiniBatchFit(model_SGDM, optimizer2,\n",
    "             X_train, Y_train, \n",
    "             output_dim, \n",
    "             num_epochs=num_epochs, \n",
    "             batch_size=batch_size, \n",
    "             learning_rate=learning_rate)\n",
    "\n",
    "# 训练模型\n",
    "SGDM_Fit.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f14f3900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 49.07%\n",
      "Validation F1 Score: 0.4784\n",
      "Validation Recall: 0.4907\n",
      "Validation Precision: 0.4950\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_train_SGDM = model_SGDM.predict_and_evaluate(X_val, Y_val)\n",
    "precision_val,recall_val,f1_val = classification_report_numpy(Y_val, predictions_train_SGDM, average='macro')\n",
    "print(f\"Validation F1 Score: {f1_val:.4f}\")\n",
    "print(f\"Validation Recall: {recall_val:.4f}\")\n",
    "print(f\"Validation Precision: {precision_val:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bde938a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Test Accuracy: 48.70%\n",
      "Test F1 Score: 0.4734\n",
      "Test Recall: 0.4870\n",
      "Test Precision: 0.4910\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictions_test_SGDM = model_SGDM.predict_and_evaluate(X_test, Y_test)\n",
    "precision_test,recall_test,f1_test = classification_report_numpy(Y_test, predictions_test_SGDM, average='macro')\n",
    "print(f\"Test F1 Score: {f1_test:.4f}\")\n",
    "print(f\"Test Recall: {recall_test:.4f}\")\n",
    "print(f\"Test Precision: {precision_test:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CXXA1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
